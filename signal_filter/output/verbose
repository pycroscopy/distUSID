[or-condo-c181.ornl.gov:42288] 1 more process has sent help message help-mpi-runtime.txt / mpi_init:warn-fork
[or-condo-c181.ornl.gov:42288] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[or-condo-c181:42307] *** Process received signal ***
[or-condo-c181:42307] Signal: Segmentation fault (11)
[or-condo-c181:42307] Signal code: Address not mapped (1)
[or-condo-c181:42307] Failing at address: 0x52c
[or-condo-c184:17656] *** Process received signal ***
[or-condo-c184:17656] Signal: Segmentation fault (11)
[or-condo-c184:17656] Signal code: Address not mapped (1)
[or-condo-c184:17656] Failing at address: 0x52c
[or-condo-c181:42307] [ 0] /lib64/libpthread.so.0(+0xf100)[0x2b07f7374100]
[or-condo-c181:42307] [ 1] /software/dev_tools/swtree/cs400_centos7.2_pe2016-08/hdf5-parallel/1.8.17/centos7.2_gnu5.3.0/lib/libhdf5.so.10(H5F_close+0xc)[0x2b07fec3723c]
[or-condo-c181:42307] [ 2] /software/dev_tools/swtree/cs400_centos7.2_pe2016-08/hdf5-parallel/1.8.17/centos7.2_gnu5.3.0/lib/libhdf5.so.10(H5I_dec_ref+0xee)[0x2b07feca235e]
[or-condo-c181:42307] [ 3] /software/dev_tools/swtree/cs400_centos7.2_pe2016-08/hdf5-parallel/1.8.17/centos7.2_gnu5.3.0/lib/libhdf5.so.10(H5I_dec_app_ref+0x22)[0x2b07feca2572]
[or-condo-c181:42307] [ 4] /software/dev_tools/swtree/cs400_centos7.2_pe2016-08/hdf5-parallel/1.8.17/centos7.2_gnu5.3.0/lib/libhdf5.so.10(H5Idec_ref+0x30)[0x2b07feca2680]
[or-condo-c181:42307] [ 5] /home/syz/.local/lib/python3.6/site-packages/h5py-2.8.0.post0-py3.6-linux-x86_64.egg/h5py/defs.cpython-36m-x86_64-linux-gnu.so(+0x21ffd)[0x2b080cb38ffd]
[or-condo-c181:42307] [ 6] /home/syz/.local/lib/python3.6/site-packages/h5py-2.8.0.post0-py3.6-linux-x86_64.egg/h5py/_objects.cpython-36m-x86_64-linux-gnu.so(+0x14ccc)[0x2b080c909ccc]
[or-condo-c181:42307] [ 7] [or-condo-c184:17656] [ 0] /lib64/libpthread.so.0(+0xf100)[0x2b3e41bd9100]
python[0x47fcc2]
[or-condo-c181:42307] [ 8] python[0x433e27]
[or-condo-c181:42307] [ 9] python[0x433e37]
[or-condo-c181:42307] [10] python(_PyEval_EvalFrameDefault+0x5055)[0x545b95]
[or-condo-c181:42307] [11] [or-condo-c184:17656] [ 1] /software/dev_tools/swtree/cs400_centos7.2_pe2016-08/hdf5-parallel/1.8.17/centos7.2_gnu5.3.0/lib/libhdf5.so.10(H5F_close+0xc)[0x2b3e4949c23c]
[or-condo-c184:17656] [ 2] python[0x53f771]
[or-condo-c181:42307] [12] python(PyEval_EvalCode+0x60)[0x5405b0]
[or-condo-c181:42307] [13] python(PyRun_FileExFlags+0x168)[0x4265f8]
[or-condo-c181:42307] [14] /software/dev_tools/swtree/cs400_centos7.2_pe2016-08/hdf5-parallel/1.8.17/centos7.2_gnu5.3.0/lib/libhdf5.so.10(H5I_dec_ref+0xee)[0x2b3e4950735e]
[or-condo-c184:17656] [ 3] /software/dev_tools/swtree/cs400_centos7.2_pe2016-08/hdf5-parallel/1.8.17/centos7.2_gnu5.3.0/lib/libhdf5.so.10(H5I_dec_app_ref+0x22)[0x2b3e49507572]
[or-condo-c184:17656] [ 4] /software/dev_tools/swtree/cs400_centos7.2_pe2016-08/hdf5-parallel/1.8.17/centos7.2_gnu5.3.0/lib/libhdf5.so.10(H5Idec_ref+0x30)[0x2b3e49507680]
[or-condo-c184:17656] [ 5] /home/syz/.local/lib/python3.6/site-packages/h5py-2.8.0.post0-py3.6-linux-x86_64.egg/h5py/defs.cpython-36m-x86_64-linux-gnu.so(+0x21ffd)[0x2b3e571bbffd]
[or-condo-c184:17656] [ 6] /home/syz/.local/lib/python3.6/site-packages/h5py-2.8.0.post0-py3.6-linux-x86_64.egg/h5py/_objects.cpython-36m-x86_64-linux-gnu.so(+0x14ccc)[0x2b3e56f8cccc]
[or-condo-c184:17656] [ 7] python[0x47fcc2]
[or-condo-c184:17656] [ 8] python[0x433e27]
[or-condo-c184:17656] [ 9] python(PyRun_SimpleFileExFlags+0xdd)[0x4267dd]
[or-condo-c181:42307] [15] python(Py_Main+0xd5a)[0x43adba]
[or-condo-c181:42307] [16] python(main+0x162)[0x41d8d2]
[or-condo-c181:42307] [17] /lib64/libc.so.6(__libc_start_main+0xf5)[0x2b07f7cabb15]
[or-condo-c181:42307] [18] python[0x41d991]
[or-condo-c181:42307] *** End of error message ***
python[0x433e37]
[or-condo-c184:17656] [10] python(_PyEval_EvalFrameDefault+0x5055)[0x545b95]
[or-condo-c184:17656] [11] python[0x53f771]
[or-condo-c184:17656] [12] python(PyEval_EvalCode+0x60)[0x5405b0]
[or-condo-c184:17656] [13] python(PyRun_FileExFlags+0x168)[0x4265f8]
[or-condo-c184:17656] [14] python(PyRun_SimpleFileExFlags+0xdd)[0x4267dd]
[or-condo-c184:17656] [15] python(Py_Main+0xd5a)[0x43adba]
[or-condo-c184:17656] [16] python(main+0x162)[0x41d8d2]
[or-condo-c184:17656] [17] /lib64/libc.so.6(__libc_start_main+0xf5)[0x2b3e42510b15]
[or-condo-c184:17656] [18] python[0x41d991]
[or-condo-c184:17656] *** End of error message ***
=>> PBS: job killed: walltime 219 exceeded limit 180
mpirun: abort is already in progress...hit ctrl-c again to forcibly terminate

/home/syz/mpi_tutorials/signal_filter
--------------------------------------------------------------------------
An MPI process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your MPI job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          or-condo-c181 (PID 42307)
  MPI_COMM_WORLD rank: 0

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
Rank 1 of 2 on or-condo-c184.ornl.gov sees 36 logical cores on the socket
Rank 0 of 2 on or-condo-c181.ornl.gov sees 36 logical cores on the socket
Working on 2 nodes via MPI
Each rank is required to work on 128 of the 256 positions in this dataset
Rank 0 will read positions 0 to 128 of 256
Allowed to read 427 pixels per chunk
Allowed to use up to 36 cores and 4096 MB of memory
Consider calling test() to check results before calling compute() which computes on the entire dataset and writes back to the HDF5 file
Allowed to read 142 pixels per chunk
Checking for duplicates:
Creating datagroup and datasets
Rank 1 will read positions 128 to 256 of 256
Consider calling test() to check results before calling compute() which computes on the entire dataset and writes back to the HDF5 file
Creating datagroup and datasets
Rank 0 - Finished creating the Composite_Filter dataset
Rank 0 - Reusing source datasets position datasets
h5 group and file OK
quantity, units, main_data_name all OK
Selected empty dataset creation. OK so far
Provided h5 position indices and values OK
Passed all pre-tests for creating spectroscopic datasets
Indices:
[[0]]
Values:
[[1.]]
Starting to write Region References to Dataset /Measurement_000/Channel_000/Raw_Data-FFT_Filtering_000/Noise_Spec_Indices of shape: (1, 1)
About to write region reference: arb : (slice(0, 1, None), slice(None, None, None))
Comparing (slice(0, 1, None), slice(None, None, None)) with h5 dataset maxshape of (1, 1)
Region reference tuple now: [slice(0, 1, None), slice(None, None, None)]
Wrote Region Reference:arb
Writing header attributes: labels
Wrote Region References of Dataset Noise_Spec_Indices
Starting to write Region References to Dataset /Measurement_000/Channel_000/Raw_Data-FFT_Filtering_000/Noise_Spec_Values of shape: (1, 1)
About to write region reference: arb : (slice(0, 1, None), slice(None, None, None))
Comparing (slice(0, 1, None), slice(None, None, None)) with h5 dataset maxshape of (1, 1)
Region reference tuple now: [slice(0, 1, None), slice(None, None, None)]
Wrote Region Reference:arb
Writing header attributes: labels
Wrote Region References of Dataset Noise_Spec_Values
Created Spectroscopic datasets
Created empty dataset for Main
Wrote quantity and units attributes to main dataset
Successfully linked datasets - dataset should be main now
Rank 0 - Finished creating the Noise_Floors dataset
You maybe able to abort this computation at any time and resume at a later time!
	If you are operating in a python console, press Ctrl+C or Cmd+C to abort
	If you are in a Jupyter notebook, click on "Kernel">>"Interrupt"
Rank 0 - Finished creating the Filtered dataset
You maybe able to abort this computation at any time and resume at a later time!
	If you are operating in a python console, press Ctrl+C or Cmd+C to abort
	If you are in a Jupyter notebook, click on "Kernel">>"Interrupt"
Rank 1 - Read positions 128 to 256. Need to read till 256
Rank 0 - Read positions 0 to 128. Need to read till 128
Number of CPU free cores set to: 2 given that the CPU has 36 logical cores.
36 cores requested.
computational jobs per core = 3. For short computations, each core must have at least 20 jobs to warrant parallel computation.
Computations are not lengthy.
Not enough jobs per core. Reducing cores to 3
Starting computing on 3 cores (requested 36 cores)
Number of CPU free cores set to: 2 given that the CPU has 36 logical cores.
36 cores requested.
computational jobs per core = 3. For short computations, each core must have at least 20 jobs to warrant parallel computation.
Computations are not lengthy.
Not enough jobs per core. Reducing cores to 3
Starting computing on 3 cores (requested 36 cores)
Finished parallel computation
Finished parallel computation
Rank 1 - parallel computed chunk in 4.29 sec or 33.52 msec per pixel
Rank 0 - parallel computed chunk in 4.4 sec or 34.38 msec per pixel
Rank 1 - Finished processing upto pixel 256 of 256
Rank 1 - Finished reading all data!
Rank 1 - Finished computing all jobs!
Rank 0 - Finished processing upto pixel 128 of 128
Rank 0 - Finished reading all data!
Rank 0 - Finished computing all jobs!